from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_core.globals import set_verbose, set_debug
from langgraph.graph import StateGraph, END
from langchain.agents import create_agent
from prompts import *
from states import *
from tools import *

# Extract API Key from .env
load_dotenv()

set_verbose(True)
set_debug(True)

# Instantiating GPT Open Source LLM with 120 billion params
llm = ChatGroq(model="openai/gpt-oss-120b")


def planner_agent(state: dict) -> dict:
    """Converts user prompt to a structured development Plan.

    The response from invoking the LLM will follow Plan, which is a pydantic
    schema outlined in states.py.
    Action points of the dev Plan are generated by subsequent architect agent.
    """
    user_prompt = state["user_prompt"]
    # response is a Plan object
    response = llm.with_structured_output(Plan).invoke(planner_prompt(user_prompt))
    if response is None:
        raise ValueError("Planner did not return a valid response.")
    return {"plan": response}


def architect_agent(state: dict) -> dict:
    """Generates TaskPlan from Plan created by planner agent."""
    plan: Plan = state["plan"]
    response = llm.with_structured_output(TaskPlan).invoke(architect_prompt(plan))
    if response is None:
        raise ValueError("Architect did not return a valid response.")
    response.plan = plan
    return {"task_plan": response}


def coder_agent(state: dict) -> dict:
    """Generates CoderState for each filepath in TaskPlan.

    Makes calls to LangGraph tools defined in tools.py.
    """
    coder_state: CoderState = state.get("coder_state")
    if coder_state is None:
        coder_state = CoderState(task_plan=state["task_plan"], current_step_idx=0)

    steps = coder_state.task_plan.implementation_steps
    if coder_state.current_step_idx >= len(steps):
        return {"coder_state": coder_state, "status": "DONE"}

    current_task = steps[coder_state.current_step_idx]
    existing_content = read_file.run(current_task.filepath)

    system_prompt = coder_system_prompt()
    user_prompt = (
        f"Task: {current_task.task_description}\n"
        f"File: {current_task.filepath}\n"
        f"Existing content:\n{existing_content}\n"
        "Use write_file(path, content) to save your changes."
    )

    coder_tools = [read_file, write_file, list_files, get_current_directory]
    react_agent = create_agent(llm, coder_tools)

    react_agent.invoke(
        {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
        }
    )

    coder_state.current_step_idx += 1
    return {"coder_state": coder_state}


graph = StateGraph(dict)
graph.add_node("planner", planner_agent)
graph.add_node("architect", architect_agent)
graph.add_node("coder", coder_agent)
graph.add_edge("planner", "architect")
graph.add_edge("architect", "coder")
graph.add_conditional_edges(
    "coder",
    lambda s: "END" if s.get("status") == "DONE" else "coder",
    {"END": END, "coder": "coder"},
)
graph.set_entry_point("planner")

agent = graph.compile()


if __name__ == "__main__":
    user_prompt = "Create a simple calculator web application."
    result = agent.invoke({"user_prompt": user_prompt}, {"recursion_limit": 100})
    # Even though state is never explicitly defined, dict within
    # .invoke() becomes the initial global state, which LangGraph
    # automatically updates with the partial dict created by
    # each subsequent node / agent.

    # State is always the cumulative merge of all partial dicts
    # returned by nodes, starting from whatever dict you pass to .invoke().
    print(result)
